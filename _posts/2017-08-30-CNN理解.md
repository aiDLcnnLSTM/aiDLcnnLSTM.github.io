---
layout: post
title: CNN理解
---
ref:
[Deep learning](http://www.cs.toronto.edu/~hinton/science.pdf)

0、神经网络 反向传播 梯度 链式法则 row data到feature
特征工程 人工提取特征

1、 CNN结构 卷积 +  Pooling 最后full_connected

2、神经网络结构2个原因：
>（1）例如图像，局部数据具有很强的相关性
>（2）分类问题 图像中的主题是位置无关的，无论猫出现在图像中什么位置，这张图都是猫。而卷积则具有平移不变性。

3、卷积 pool作用
>(1)卷积 CNN
> 
> the role of the convolutional layer is to detect local conjunctions of features from the previous layer
>
>(2) pooling
>
>role of the pooling
layer is to merge semantically similar features into one
>
>refs:
>[http://www.360doc.com/content/16/0418/16/2036337_551674375.shtml](http://www.360doc.com/content/16/0418/16/2036337_551674375.shtml)
>
>[http://blog.csdn.net/mao_kun/article/details/50507376](http://blog.csdn.net/mao_kun/article/details/50507376)
>
>[CNN网络的pooling层有什么用？](www.zhihu.com)
>
>[http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)

>Learning Mid-Level Feature For Recognition
>A Theoretical Analysis of Feature Pooling in Visual Recognition
>
>Comparison of Mid-Level Feature Coding Approaches And Pooling Strategies in Visual Concept Detection
>
>Protected Pooling Method Of Sparse Coding In Visual Classi cation



>1.引入了位移不变性，更关注是否存在某些特征而不是特征具体的位置。比如最常见的max pooling，因为取一片区域的最大值，所以这个最大值在该区域内无论在哪，max-pooling之后都是它，相当于对微小位移的不变性。（知乎详细解释）

>2.减小下一层输入大小，减小计算量和参数个数，这个作用是最直观的了。

>3. 获得定长输出。改变输出的维度。

>4. 防止过拟合或有可能会带来欠拟合。减少参数，减少过拟合的风险。


#####Deep Learning 简介
[http://blog.csdn.net/abcjennifer/article/details/7826917/](http://blog.csdn.net/abcjennifer/article/details/7826917/)

[http://www.cnblogs.com/zeze/p/6801831.html](http://www.cnblogs.com/zeze/p/6801831.html)

[http://www.cnblogs.com/zeze/p/6801831.html](http://www.cnblogs.com/zeze/p/6801831.html)