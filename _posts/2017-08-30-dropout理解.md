---
layout: post
title: dropout理解
---

[http://blog.csdn.net/hjimce/article/details/50413257](http://blog.csdn.net/hjimce/article/details/50413257)

[http://www.datakit.cn/blog/2014/08/14/Dropout_for_deep_learning.html](http://www.datakit.cn/blog/2014/08/14/Dropout_for_deep_learning.html)


This shows that dropout does break up co-adaptations, which is probably the main reason why it leads to lower generalization errors.

1. 加入随机的dropout,相等于训练了很多子模型，然后综合各个子模型，提高泛化能力。
2. Dropout可以防止过拟合，提高范化能力。
3. dropout设置 对于input p=0.8 对于hiddenlayer p=[0.5~0,8]
4. 注意p的意思
5. （1）在论文中 一般的 p表示留下的概率，p=1表示不丢掉神经元
6. （2）在keras中 Dropout 层，表示要丢掉的概率
7. （3）在tensorf中 
	tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
    keep_prob: A scalar `Tensor` with the same type as x. The probability that each element is kept.
